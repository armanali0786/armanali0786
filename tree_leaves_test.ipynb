{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armanali0786/armanali0786/blob/main/tree_leaves_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tHmKTukFxoaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "321b2055-90bd-4ae9-88c6-fcecc1b106df"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-68ccdfc3ee40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#print(len(classes))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Load the dataset with Image Folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;31m#print(trainset.targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    311\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    144\u001b[0m                                             target_transform=target_transform)\n\u001b[1;32m    145\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0;34m\"The class_to_idx parameter cannot be None.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             )\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mmake_dataset\u001b[0;34m(directory, class_to_idx, extensions, is_valid_file)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mextensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"Supported extensions are: {', '.join(extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Found no valid file for the classes Ashawagandha_Arman, Babul_Train, DhaturoTree_Arman, marjoram_Train, sonflower_Train. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp"
          ]
        }
      ],
      "source": [
        "# Importing Numpy Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "# import torch.utils.data as data\n",
        "# from torch.utils.data import Dataset\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "if __name__ == '__main__':\n",
        "    # data_dir = \"/content/drive/My Drive/Cat_Dog_Imageset\"\n",
        "    # train_dir = data_dir + '/training_set' # training_set contains training dataset\n",
        "    # test_dir = data_dir + '/test_set'  #contains test dataset\n",
        "    train_dir = '/content/drive/MyDrive/Tree and Grass Information/train'\n",
        "    test_dir = '/content/drive/MyDrive/Tree and Grass Information/test'\n",
        "\n",
        "# Defining transformations for training and test data\n",
        "# transforms.compose() will apply transformation to images\n",
        "    transformation = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                         transforms.RandomRotation(20),\n",
        "                                         transforms.Resize(size=(224, 224)),\n",
        "                                         transforms.ToTensor(),\n",
        "                                         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                         ])\n",
        "    classes = sorted(os.listdir(train_dir))\n",
        "    #print(len(classes))\n",
        "    # Load the dataset with Image Folder\n",
        "    trainset = datasets.ImageFolder(train_dir, transform=transformation)\n",
        "    #print(trainset.targets)\n",
        "    testset = datasets.ImageFolder(test_dir, transform=transformation)\n",
        "\n",
        "    # define data loaders\n",
        "    batch_size = 32\n",
        "    train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(testset, batch_size=batch_size, num_workers=1)\n",
        "\n",
        "    # def imshow(img):\n",
        "    #     img = img / 2 + 0.5  # unnormalize\n",
        "    #     plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
        "    #     #plt.show()\n",
        "    # # obtain one batch of training images\n",
        "    # data_iter = iter(train_loader)\n",
        "    # images, lbls = data_iter.next()\n",
        "    # images = images.numpy() # convert images to numpy for display\n",
        "    # # plot the images in the batch, along with the corresponding labels\n",
        "    # fig = plt.figure(figsize=(10, 4))\n",
        "    # # display 20 images\n",
        "    # for idx in np.arange(10):\n",
        "    #     ax = fig.add_subplot(2, 10/2, idx+1, xticks=[], yticks=[])\n",
        "    #     imshow(images[idx])\n",
        "    #     label = lbls[idx]\n",
        "    #     #ax.set_title(classes[label])\n",
        "    #     #ax.set_title(classes[lbls[idx]])\n",
        "    # Creating CNN classifier\n",
        "    train_on_gpu = torch.cuda.is_available()  # check if Cuda is available for training\n",
        "\n",
        "\n",
        "    # Initializing Parameters\n",
        "    class Net(nn.Module):\n",
        "\n",
        "        def __init__(self):\n",
        "            super(Net, self).__init__()\n",
        "            # convolutional layer1\n",
        "            self.conv1 = nn.Conv2d(3, 16, 5)\n",
        "            # max pooling layer\n",
        "            self.pool = nn.MaxPool2d(2, 2)\n",
        "            # convolutional layer2\n",
        "            self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "            self.dropout = nn.Dropout(0.2)\n",
        "            # Fully connected layer1\n",
        "            self.fc1 = nn.Linear(32 * 53 * 53, 256)\n",
        "            # fully connected layer2\n",
        "            self.fc2 = nn.Linear(256, 84)\n",
        "            # fully connected layer3\n",
        "            self.fc3 = nn.Linear(84, len(classes))\n",
        "            # Applying softmax function\n",
        "            self.softmax = nn.LogSoftmax(dim=1)\n",
        "            # feed forward network\n",
        "\n",
        "        def forward(self, x):\n",
        "            # add sequence of convolutional and max pooling layers\n",
        "            x = self.pool(F.relu(self.conv1(x)))\n",
        "            x = self.pool(F.relu(self.conv2(x)))\n",
        "            x = self.dropout(x)\n",
        "            x = x.view(-1, 32 * 53 * 53)\n",
        "            x = F.relu(self.fc1(x))\n",
        "            x = self.dropout(F.relu(self.fc2(x)))\n",
        "            x = self.softmax(self.fc3(x))\n",
        "            return x\n",
        "\n",
        "            # create Model instance\n",
        "\n",
        "\n",
        "    model = Net()\n",
        "    print(model)\n",
        "\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if (train_on_gpu):\n",
        "        model.cuda()\n",
        "        print(\"CUDA available\")\n",
        "    # specify loss function\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    # specify optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
        "    # Train Model\n",
        "    # number of epochs to train the model\n",
        "    n_epochs = 20  # you may increase this number to train a final model\n",
        "\n",
        "    # valid_loss_min = np.Inf # track change in validation loss\n",
        "    train_acc= []\n",
        "    loss_values = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "        # keep track of training and validation loss\n",
        "\n",
        "        train_loss = 0.0\n",
        "        #valid_loss = 0.0\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            # move tensors to GPU if CUDA is available\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the batch loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # update training loss\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "            # print statistics\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct += torch.sum(pred == target).item()\n",
        "            total += target.size(0)\n",
        "        # calculate average losses\n",
        "        train_loss = train_loss / len(train_loader.dataset)\n",
        "        loss_values.append(train_loss)\n",
        "        train_acc.append(100 * correct / total)\n",
        "        # print training/validation statistics\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
        "            epoch, train_loss))\n",
        "    print(\"This is accuracy\",train_acc)\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    plt.title(\"Train - Validation Loss\")\n",
        "    plt.plot(loss_values, label='train')\n",
        "    #plt.plot(val_loss, label='validation')\n",
        "    plt.xlabel('num_epochs', fontsize=12)\n",
        "    plt.ylabel('loss', fontsize=12)\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 10))\n",
        "    plt.title(\"Train - Validation Accuracy\")\n",
        "    plt.plot(train_acc, label='train')\n",
        "    #plt.plot(val_acc, label='validation')\n",
        "    plt.xlabel('num_epochs', fontsize=12)\n",
        "    plt.ylabel('accuracy', fontsize=12)\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    # Device configuration\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    # Test the model\n",
        "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(correct)\n",
        "        print(total)\n",
        "        print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "    # Save the model checkpoint\n",
        "    torch.save(model.state_dict(), 'model.pth')\n",
        "\n",
        "    network = Net()\n",
        "    network.load_state_dict(torch.load('model.pth'))\n",
        "\n",
        "    img = Image.open(\"/content/drive/MyDrive/Tree and Grass Information/test/Ashwagandha_test_Arman/1 (1).jfif\")\n",
        "    img_t = transformation(img)\n",
        "    batch_t = torch.unsqueeze(img_t, 0)\n",
        "    print(\"this is shape\", batch_t.shape)\n",
        "    network.eval()\n",
        "    out = network(batch_t)\n",
        "    print(out.shape)\n",
        "    prediction = int(torch.max(out.data, 1)[1].numpy())\n",
        "    print(prediction)\n",
        "    print(\"this is class\",classes[prediction])\n",
        "    # if prediction == 0:\n",
        "    #     print('limdo')\n",
        "    # if prediction == 1:\n",
        "    #     print('piplo')\n",
        "    # if prediction == 2:\n",
        "    #     print('vad')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "S7e_6XOYxw0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf414481-cc4c-462d-d418-d6005d3eb156"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}